{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "<img src='../../resources/deep_learning/dl.png' />\n",
    "\n",
    "* Mimic how the human brain operates\n",
    "    * Input Values (Input Layer)\n",
    "    * Hidden Layer\n",
    "    * Output Value (Output Layer)\n",
    "<img src='../../resources/deep_learning/ann/dl1.png' />\n",
    "<img src='../../resources/deep_learning/ann/dl2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANN)\n",
    "\n",
    "* The Neuron\n",
    "* The Activation Function\n",
    "* How do Neural Networks work?\n",
    "* How do Neural Networks learn?\n",
    "* Gradient Descent\n",
    "* Stochastic Gradient Descent\n",
    "* Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../resources/deep_learning/ann/ann1.png' />\n",
    "\n",
    "* Input layer contains **independent variable**\n",
    "    * Make sure to standardize them\n",
    "        * Mean of 0 & variance of 1\n",
    "    * Sometimes normalize them\n",
    "    \n",
    "<img src='../../resources/deep_learning/ann/ann2.png' />\n",
    "\n",
    "* Output values can be:\n",
    "    * Continuous (price)\n",
    "    * Binary (Yes/No)\n",
    "    * Categorical - **Results in `mutiple` output values**\n",
    "    \n",
    "<img src='../../resources/deep_learning/ann/ann3.png' />\n",
    "\n",
    "* Synapses - **Weights**\n",
    "    * Weights: How Neural Networks Learn\n",
    "        * Assigns what signals are important & what not\n",
    "        * The things that get `adjusted` for the training\n",
    "            * where gradient descent & backward propagation comes into play.\n",
    "            \n",
    "<img src='../../resources/deep_learning/ann/ann4.png' />\n",
    "\n",
    "* What happens inside the `neuron`?\n",
    "    1. All of the input values get added up\n",
    "    2. Then, applies `activation` function.\n",
    "\n",
    "<img src='../../resources/deep_learning/ann/ann5.png' />\n",
    "<img src='../../resources/deep_learning/ann/ann6.png' />\n",
    "\n",
    "* Then, the neuron passes on the signal.\n",
    "\n",
    "<img src='../../resources/deep_learning/ann/ann7.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "* The signal that is being passed on.\n",
    "<img src='../../resources/deep_learning/ann/ann8.png' />\n",
    "\n",
    "\n",
    "* 4 predominant types of activation functions:\n",
    "1. Threshold Function (Yes/No type of function)\n",
    "    * x-axis: weighted sum\n",
    "    * y-axis: values 0-1\n",
    "    * Threshold function passes:\n",
    "        * x >= 0, then 1\n",
    "        * x < 0,  then 0\n",
    "<img src='../../resources/deep_learning/ann/ann9.png' />    \n",
    "\n",
    "2. Sigmoid Function\n",
    "   * 1 / (1 + e^-x)\n",
    "   * x-axis: weighted sum\n",
    "   * It is smooth & gradual progression\n",
    "       * Very useful in the output layer\n",
    "       * Especially for predicting probabilities.\n",
    "    \n",
    "<img src='../../resources/deep_learning/ann/ann10.png' /> \n",
    "\n",
    "3. Rectifier Function\n",
    "   * One of the most popular function for ANN\n",
    "   * Goes all the way to 0 at 0, and increases from there.\n",
    "\n",
    "<img src='../../resources/deep_learning/ann/ann11.png' /> \n",
    "\n",
    "4. Hyperbolic Tangent (tanh)\n",
    "   * Similar to sigmoid, but goes below 0\n",
    "       * Goes from -1 to 0, then 0 to 1\n",
    "\n",
    "<img src='../../resources/deep_learning/ann/ann12.png' /> \n",
    "\n",
    "\n",
    "\n",
    "* Deciding which function to use for Dependent Variable 0 or 1\n",
    "    * Sigmoid: Tells the probability of y being 1\n",
    "    \n",
    "<img src='../../resources/deep_learning/ann/ann13.png' /> \n",
    "\n",
    "* A function being passed in at `Hidden Layer`, then a function being passed in at `Output layer`, then yields an output.\n",
    "\n",
    "<img src='../../resources/deep_learning/ann/ann14.png' /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do Neural Networks work?\n",
    "\n",
    "* Pretend a NN is **trained**, using housing price as example.\n",
    "    * Input Layer: X1(Area), X2(Bedrooms), X3(Distance to city), X4(Age)\n",
    "    * Output Layer: Price that we are predicting\n",
    "        * Input Layer will be weighted up by synapses\n",
    "        * Output Layer will be calculated\n",
    "            * Price = w1x1 + w2x2 + w3x3 + w4x4 (Weighted sum of all of the inputs)\n",
    "            \n",
    "        <img src='../../resources/deep_learning/ann/ann15.png' /> \n",
    "        \n",
    "        * Hidden Layer gives extra power\n",
    "            * All inputs passed with weights, but **not** all inputs are value - some are 0 some are not 0\n",
    "            * This neuron might be looking for a specific relevant thing, a property not so far from the city & the size.\n",
    "            * The activation function will fire up only when certain criteria is met.\n",
    "            \n",
    "        <img src='../../resources/deep_learning/ann/ann16.png' /> \n",
    "        \n",
    "        * A neuron might also pick up only 1 variable\n",
    "            * If a property > 100y, deemd as historic & fired up\n",
    "            * Good example of **rectifier function**\n",
    "            \n",
    "        <img src='../../resources/deep_learning/ann/ann17.png' /> \n",
    "        \n",
    "        * Together, they have strong power to predict the price\n",
    "        \n",
    "        <img src='../../resources/deep_learning/ann/ann18.png' /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do Neural Network learn?\n",
    "\n",
    "* Avoid putting in the rule, and let the program learn by itself.\n",
    "\n",
    "* y^ is the output value; y is the actual value\n",
    "\n",
    "<img src='../../resources/deep_learning/ann/ann19.png' /> \n",
    "\n",
    "* Let's say inputs applied, then activation function applied, and an output y^ is given. We compare it to actual value y.\n",
    "    1. Calculate Cost Function: C = 1/2(y^-y)^2\n",
    "        * Tells the error in the prediction.\n",
    "        * The goal is to `minimize` the cost function.\n",
    "    2. We then feed the info on cost function back to the neural network.\n",
    "    3. Goes back to the weights, and the weights get updated.\n",
    "        * The only thing we have in control is the weights (W1 - Wm)\n",
    "    4. Repeat & adjust the weights, changes y^\n",
    "        \n",
    "    * **Note:** We are only dealing with dataset of **one row**.\n",
    "        <img src='../../resources/deep_learning/ann/ann20.png' /> \n",
    "        \n",
    "* Let's see **multiple rows**\n",
    "    * 1 epoch is when we go through the whole dataset & train the neural network on all of these rows.\n",
    "    * After getting **all of the values & compare with actual values**, we can calculate the **cost function** of all: **sum of 1/2(y^ - y)^2**\n",
    "    * We then go back and update all.\n",
    "    * **Note:** All of these are 1 neural network. All of the rows **Share the weights**.\n",
    "\n",
    "\n",
    "* The goal is to minimize the cost function - find optimal weights\n",
    "    * This is **Backpropagation**\n",
    "<img src='../../resources/deep_learning/ann/ann21.png' /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "* In order for a Neural Network to learn, we use **backpropagation**, and adjust the weights accordingly.\n",
    "\n",
    "* How the weight is adjusted/minimize the cost function:\n",
    "    * One of the way is the: Brute Force method\n",
    "    * <img src='../../resources/deep_learning/ann/ann22.png' /> \n",
    "\n",
    "* **But**, if we increase the number of weights, we will encounter **Curse of Dimensionality**\n",
    "    * Before a neural network is trained:\n",
    "    * <img src='../../resources/deep_learning/ann/ann23.png' /> \n",
    "    \n",
    "    * It takes **way too long, unrealistic**.\n",
    "    \n",
    "    * <img src='../../resources/deep_learning/ann/ann24.png' /> \n",
    "    \n",
    "    \n",
    "* Therefore, we use **gradient descent**\n",
    "    *  We first find a point where the cost function starts\n",
    "        * We then look at the **angle** of cost function (gradient).\n",
    "        * If **slope = -ve**, it is **going downhill**, goes right.\n",
    "        * If **slope = +ve**, it is **going uphill**, goes left.\n",
    "            * Repeat & find the **best weights** (Like ball rolling)\n",
    "            * Basically, see which way it is going downwards.\n",
    "            \n",
    "    * <img src='../../resources/deep_learning/ann/ann25.png' /> \n",
    "    \n",
    "    * Gradient descent in **two-dimensional** space.\n",
    "        * Descending into the **minimum** of the cost function.\n",
    "    \n",
    "    * <img src='../../resources/deep_learning/ann/ann26.png' /> \n",
    "    \n",
    "    * Gradient descent in **three-dimensional** space.\n",
    "    * <img src='../../resources/deep_learning/ann/ann27.png' /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "* For a regular gradient descent, we require the Cost Function is `convex`.\n",
    "\n",
    "* What if it looks:\n",
    "    * We might found a wrong one for non-convex cost function.\n",
    "    * <img src='../../resources/deep_learning/ann/ann28.png' /> \n",
    "    \n",
    "    * We need Stochastic Gradient Descent\n",
    "        * Does not require the cost function to be convex.\n",
    "        * We take the rows one by one, unlike the regular gradient descent, known as batch gradient descent.\n",
    "        * Take the row one by one & adjust the weights. Adjust the weights after **every single row**.\n",
    "        * <img src='../../resources/deep_learning/ann/ann29.png' /> \n",
    "        \n",
    "    * Stochastic Gradient Descent:\n",
    "        * Helps avoid local minimum instead of `global minimum`.\n",
    "        * Much higher fluctuation - one iteration/one row at a time.\n",
    "        * It is **faster** as it does not have to load up data in the memory by doing one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "* Forward Propagation\n",
    "    * Information entered into the input layer & propagated forward.\n",
    "    * Get output value y^ & compare to the actual value y.\n",
    "    * Then, calculated the error.\n",
    "    \n",
    "* Backpropagation\n",
    "    * The error is then backpropagated, allows us to adjust the weights to train the network.\n",
    "    * Advanced algorithm which allows to adjust all of the weights simultaneously. \n",
    "    * During the process, because of the way the algorithm is structured, we are able to adjust all of the weights at the same time.\n",
    "        * Know which part of the error each of the weights in the network is responsible for.\n",
    "\n",
    "\n",
    "**Steps: Training the ANN with Stochastic Gradient Descent**\n",
    "\n",
    "1. Randomly initialise the weights to small numbers close to 0, but not 0.\n",
    "\n",
    "2. Input the first observation of the dataset in the input layer, each feature in one input node.\n",
    "\n",
    "3. Forward Propagation: from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result y^. (The weights determine how important each neuron activation is)\n",
    "\n",
    "4. Compare the predicted result to the actual result. Measure the generated error.\n",
    "\n",
    "5. Back Propagation: from right to left, the error is back propagatted. Update the weights accoridng to how much they are responsible for the error. The learning rate decides how much we update the weights.\n",
    "\n",
    "6. Repeat Steps 1 - 5 & update the weights after each observation (Reinforcement Learning/Stochastic GD). Or Repeat Steps 1 - 5 but update the weights only after a batch of observations (Batch Learning/Batch GD/ Mini Batch GD).\n",
    "\n",
    "7. When whole training set passed through the ANN, that makes an epcoh. Redo more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science_playground]",
   "language": "python",
   "name": "conda-env-data_science_playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
