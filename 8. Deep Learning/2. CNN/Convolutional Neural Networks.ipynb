{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "* For images.\n",
    "\n",
    "* How it works:\n",
    "    * Input Image -> CNN -> Output Label (Image Class)\n",
    "\n",
    "* How computer reads images:\n",
    "    * Digital representation\n",
    "    * B/W (2D Array) - 0 will be completely black pixels & 255 will be completely white pixel; Gray scale in between.\n",
    "    * Coloured (3D Array) - Pixel has 3 values assigned to it, each between 0 & 255\n",
    "        * Find out the colour by combining  the values\n",
    "        * Red Channel, Green Channel, Blue Channel\n",
    "        \n",
    "<img src='../../resources/deep_learning/cnn/cnn1.png' />\n",
    "\n",
    "* For example, very simply (Assume 0 is white & 1 is black):\n",
    "<img src='../../resources/deep_learning/cnn/cnn2.png' />\n",
    "\n",
    "* Steps:\n",
    "1. Convolution\n",
    "2. Max Pooling\n",
    "3. Flattening\n",
    "4. Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolution\n",
    "\n",
    "* Function:\n",
    "    * Basically combine integration of two functions\n",
    "<img src='../../resources/deep_learning/cnn/cnn3.png' />\n",
    "\n",
    "* In simplified terms with pixels only in 0 & 1, and feature detector 3x3 (doesn't have to be 3x3):\n",
    "    * Feature Detector can be called: Kernel/ Filter\n",
    "    * Convolution operation is signified by a X in a circle.\n",
    "    * Take the filter & put on the image, and multiple each value (element-wise) & add up to give a Feature Map\n",
    "    * Feature Map can be known as Convolved Feature/ Activation Map.\n",
    "    \n",
    "<img src='../../resources/deep_learning/cnn/cnn4.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn5.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn6.png' />\n",
    "\n",
    "* Reduced the size of the image - This step is to make the image smaller to process faster\n",
    "* Convolutional Layer: Create multiple feature maps to get first convolution layer\n",
    "    * We use different filters, use certain features & not just one.\n",
    "    * To get feature A, we use feature detector A, to get B, we use another detetor.\n",
    "\n",
    "<img src='../../resources/deep_learning/cnn/cnn7.png' />\n",
    "\n",
    "* For photo filters, we use different feature detector (eg. sharpen) to get different feature map of the image (eg. sharpened image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(B) ReLU Layer\n",
    "\n",
    "* For the Convolutional Layer, we apply Rectifier to increase non-linearity in the image.\n",
    "    * This is because images themselves are highly non-linear.\n",
    "    * But when we apply mathematical operation like convolution, we might risk increasing linearity, so we need to break them up.\n",
    "<img src='../../resources/deep_learning/cnn/cnn8.png' />\n",
    "\n",
    "* For example: From a image to black & white, then takes out the black part.\n",
    "    * From white to gray, the next step will be black - linear concept.\n",
    "    * With this, introduce non-linearity.\n",
    "<img src='../../resources/deep_learning/cnn/cnn11.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn9.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn10.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "\n",
    "* We want Neural Networks to recognize:\n",
    "    * We want to make sure Neural Network to recognize the image from ALL angles - eg. cheetahs at all angles & lights\n",
    "    * Pooling - to make sure our Neural Network has flexibility to understand it.\n",
    "    \n",
    "<img src='../../resources/deep_learning/cnn/cnn12.png' />\n",
    "\n",
    "* We take a box of 2x2 (doesn't have to be), find the maximum value in that box, and note that value.\n",
    "    * We still able to preserve the feature & accounts for possible distortion & reducing the size, thus reducing number of parameters -> preventing overfitting.\n",
    "    * Because we are taking the max pooling, for the eyes feature at a certain position, we still are getting the same pool feature max.\n",
    "    \n",
    "<img src='../../resources/deep_learning/cnn/cnn13.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn15.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn14.png' />\n",
    "\n",
    "<img src='../../resources/deep_learning/cnn/cnn16.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "\n",
    "* After flattening, will then be our input layer of Artificial Neural Network.\n",
    "<img src='../../resources/deep_learning/cnn/cnn17.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn18.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn19.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection\n",
    "\n",
    "* Adding a whole Artificial Neural Network to Convolutional Neural Network.\n",
    "\n",
    "<img src='../../resources/deep_learning/cnn/cnn20.png' />\n",
    "\n",
    "* Hidden Layer called Fully Connected Layer as it is a more specific type of hidden layer.\n",
    "    * In Artificial Neural Network, hidden layers don't have to be fully connected.\n",
    "    * In Convolutional Neural Network, we use fully connected hidden layer.\n",
    "    \n",
    "* After flattening, we have features that can already perform classifications. But we use ANN to make it even better.\n",
    "\n",
    "<img src='../../resources/deep_learning/cnn/cnn21.png' />\n",
    "\n",
    "* Weights (Synapses, the blue line) is adjusted during back propagation.\n",
    "\n",
    "* How do two neurons at output layers work?\n",
    "    * Find out which of the important neurons are for the Dog.\n",
    "    * Same for the Cat.\n",
    "    * Seeing which Neuron is fired up depending on the input image.\n",
    "    \n",
    "<img src='../../resources/deep_learning/cnn/cnn22.png' />\n",
    "<img src='../../resources/deep_learning/cnn/cnn23.png' />\n",
    "\n",
    "**Note:** One output is for predicting numerical value. In this case, we have 2 outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Input Image\n",
    "2. Apply multiple feature detectors to get multiple feature maps.\n",
    "3. Then, at convolutional layer, we apply ReLU (Rectified Linear Unit) to remove any linearity.\n",
    "4. Apply pooling layer to convolutional layer (make sure the flexibility to detect the image & reduce the size & avoid overfitting & preserved the main features).\n",
    "5. Flattened all the pooled images\n",
    "6. Input into Artificial Neural Networks.\n",
    "\n",
    "<img src='../../resources/deep_learning/cnn/cnn24.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax & Cross-Entropy\n",
    "\n",
    "* Normally, the output don't add up to one, but with Softmax function applied, they add up to 1.\n",
    "<img src='../../resources/deep_learning/cnn/cnn25.png' />\n",
    "\n",
    "* Softmax comes hand in hand with Cross-Entropy function:\n",
    "    * Different version of cross-entropy function, but gives the same result.\n",
    "<img src='../../resources/deep_learning/cnn/cnn26.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing on training sets**\n",
    "\n",
    "* Transformations on the images too avoid overfitting\n",
    "* Technical terms = Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory( # Connects Image Augmentation tool to the training set\n",
    "        '../../codes_datasets/cnn_dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing on testing sets**\n",
    "\n",
    "* Rescale the testing match the scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "        '../../codes_datasets/cnn_dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialzing the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential() # Initialize cnn as a sequence instead of computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense class for fully connected layer\n",
    "# Filters specify the number of filters we want, classic architecture = 32\n",
    "# Kernel size specifies the number of rows & columns of filters/ feature detector\n",
    "# As we reshaped our images, we need to adjust input shape\n",
    "#  coloured images = (x, x, 3), grayscale iamges = (x, x, 1)\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,\n",
    "                               kernel_size=3,\n",
    "                               activation='relu',\n",
    "                               input_shape=[64, 64, 3])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply max pooling in this case\n",
    "# 2 by 2 pooling filter, strides = shift every two pixels of the filter\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding second convolutional layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,\n",
    "                               kernel_size=3,\n",
    "                               activation='relu')) # Input shape is only added when first add the images to the layer\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Full Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units = number of hidden neurons\n",
    "# Recommend for classification to use relu before reaching the output layer\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Output Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output neurons = 1\n",
    "# Sigmoid to give probability of predicted class\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'adam' optimizer to perform Stochastic Gradient Descent\n",
    "# Entropy loss for loss function as we are doing binary classification\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & Evaluating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 53s 207ms/step - loss: 0.6456 - accuracy: 0.6161 - val_loss: 0.5799 - val_accuracy: 0.7055\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.5696 - accuracy: 0.7013 - val_loss: 0.5186 - val_accuracy: 0.7460\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.5297 - accuracy: 0.7365 - val_loss: 0.5442 - val_accuracy: 0.7325\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.5217 - accuracy: 0.7350 - val_loss: 0.5274 - val_accuracy: 0.7405\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.4963 - accuracy: 0.7631 - val_loss: 0.4926 - val_accuracy: 0.7635\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 45s 182ms/step - loss: 0.4681 - accuracy: 0.7747 - val_loss: 0.5041 - val_accuracy: 0.7650\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.4601 - accuracy: 0.7803 - val_loss: 0.4626 - val_accuracy: 0.7855\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.4386 - accuracy: 0.7971 - val_loss: 0.4959 - val_accuracy: 0.7780\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.4213 - accuracy: 0.8034 - val_loss: 0.4558 - val_accuracy: 0.7910\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.4106 - accuracy: 0.8065 - val_loss: 0.4715 - val_accuracy: 0.7855\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.4057 - accuracy: 0.8123 - val_loss: 0.4655 - val_accuracy: 0.7850\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.3910 - accuracy: 0.8227 - val_loss: 0.4664 - val_accuracy: 0.7860\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 0.3772 - accuracy: 0.8285 - val_loss: 0.4744 - val_accuracy: 0.7940\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.3575 - accuracy: 0.8404 - val_loss: 0.4514 - val_accuracy: 0.7965\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.3514 - accuracy: 0.8426 - val_loss: 0.4435 - val_accuracy: 0.8140\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.3362 - accuracy: 0.8515 - val_loss: 0.4627 - val_accuracy: 0.8040\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.3297 - accuracy: 0.8569 - val_loss: 0.4870 - val_accuracy: 0.8025\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.3193 - accuracy: 0.8618 - val_loss: 0.4513 - val_accuracy: 0.8060\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.3014 - accuracy: 0.8731 - val_loss: 0.4998 - val_accuracy: 0.8105\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.2859 - accuracy: 0.8781 - val_loss: 0.4570 - val_accuracy: 0.8070\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.2673 - accuracy: 0.8911 - val_loss: 0.5010 - val_accuracy: 0.7950\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.2525 - accuracy: 0.8942 - val_loss: 0.5275 - val_accuracy: 0.7845\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.2515 - accuracy: 0.8955 - val_loss: 0.5720 - val_accuracy: 0.7935\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.2332 - accuracy: 0.9030 - val_loss: 0.4875 - val_accuracy: 0.8090\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.2228 - accuracy: 0.9097 - val_loss: 0.5637 - val_accuracy: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc4f239900>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates at the same time\n",
    "cnn.fit(x=train_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "# Needs to be in the same size\n",
    "test_image = load_img('../../codes_datasets/cnn_dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64))\n",
    "\n",
    "# Convert PIL format into 2D array\n",
    "test_image = img_to_array(test_image)\n",
    "\n",
    "# We trained our cnn with batches, so also needs to be in the same format\n",
    "# Adding dimensions, axis=0 to make sure we are adding to the first dimension\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "train_set.class_indices # Get the right class indices, dog=1, cat=0\n",
    "\n",
    "# Inside the first batch, then first image\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science_playground]",
   "language": "python",
   "name": "conda-env-data_science_playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
