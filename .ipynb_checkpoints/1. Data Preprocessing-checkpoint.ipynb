{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Process\n",
    "\n",
    "1. Data Pre-processing\n",
    "    * Import data\n",
    "    * Clean data\n",
    "    * Split into training & test sets\n",
    "        * Usually 80% train, 20% test\n",
    "    \n",
    "\n",
    "2. Modelling\n",
    "    * Build model\n",
    "    * Train model\n",
    "    * Make predictions\n",
    "    \n",
    "    \n",
    "3. Evaluation\n",
    "    * Calculate performance metrics\n",
    "    * Make verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature Scaling is always applied to columns, **NEVER** a row.\n",
    "\n",
    "2 main types:\n",
    "1. Normalization\n",
    "    * Subtract **every** value in a column by **minimum** of that column\n",
    "    * Then, divide by **difference between maximum & minimum**\n",
    "    * End up with values between **0 & 1**\n",
    "    \n",
    "    \n",
    "2. Standardization\n",
    "    * Similar, but subtract by **average**\n",
    "    * Then, divide by **standard deviation**\n",
    "    * End up with values between **-3 & 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resources/preprocessing/feature_scaling.png' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might think 2000 is way larger than 3, we might group purple & red together. **But we do not want this.**\n",
    "\n",
    "**So, we need to normalize it**\n",
    "\n",
    "They are two different things (2 columns), so it is **important to scale the features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resources/preprocessing/feature_scaling_example.png' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalization,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resources/preprocessing/normalization_example.png' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependent variable = the variable that you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/1.preprocessing/data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc` gets values at **label of index**\n",
    "\n",
    "`iloc` gets value at **location of index** -> despite of index label on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Purchased', axis=1).values\n",
    "y = df['Purchased'].values\n",
    "\n",
    "# iloc - takes value of specified rows, then specified columns\n",
    "# X = df.iloc[:, :-1].values # -> [rows, columns]\n",
    "# y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "\n",
    "1. We can remove the **entire** data set, **IF** data set it **large**.\n",
    "2. Replace the missing values by average/median/most frequent\n",
    "\n",
    "Average is most classic, but median is better as it is less affected by outliers (if data outliers are **LARGE**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imputer.fit(X[:, 1:3]) # Connect imputer with X with ALL columns BUT ONLY Numerical\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # Does the action, replace the missing values with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data\n",
    "\n",
    "As countries have no order correlation, we cannot simply encode them into 0, 1, 2.\n",
    "   * To avoid this, we can use `one hot encoding`.\n",
    "        * Turning country column into 3 columns (as 3 countries in total).\n",
    "        * Eg. France - vector (1,0,0), Germany (0,1,0) etc.\n",
    "        \n",
    "But for binary outcome (y), 0 & 1 is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode Features Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# For ColumnTransformer, the argument=[('name', EncoderClass, Column of X)], remainder='what to do for columns not encoded'\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "X = np.array(ct.fit_transform(X)) # Does fit & transform columns at the same time & force to np array type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode Labels Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y) # Dependent variable vector does not have to be numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset - Training & Test Set\n",
    "\n",
    "Despite many discussion, but `Feature Scaling` comes after splitting the dataset.\n",
    "\n",
    "`Feature Scaling` make sure all features on the same scale, and avoid one feature dominates the other.\n",
    "   * The `Test Set` suppose to be a **Brand New Set**, treat is as something **Not suppose to work with yet**.\n",
    "       * Otherwise, the scaling includes test set - information leakage.\n",
    "       * In fact, we need to fit the test set to the same scale using mean & s.d. of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Or we can add random_state=1 in argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "To avoid some features dominated other features,and not even considered by ML models.\n",
    "\n",
    "We **DO NOT** need to apply feature scaling for all ML models, but only **some**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resources/preprocessing/feature_scaling.png' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization**\n",
    "   * Subtracting each value of the feature by the mean of all values of the feature & divide by standard deviation\n",
    "\n",
    "**Normalization**\n",
    "   * Subtracting each value of the feature by the minimum of all values, divide by (difference of max & min of the feature)\n",
    "   \n",
    "**Which to apply?**\n",
    "   * `Normalization` is recommended when having a `normal distribution` in most of the features. (specific situation)\n",
    "   * `Standardization` works well `all the time`. (go for standardization, as it always works well in training process)\n",
    "   \n",
    "**Do we have to apply feature scaling to dummy variables?**\n",
    "   * No, the goal of feature scaling is to have the features in the same range, and  dummy variables are already at the same scale.\n",
    "   * Lose information if we apply on dummy variables. eg: matrix already between 0 & 1.\n",
    "   * **Only apply** feature scaling to numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For test set, we ONLY transform, as we need to `scale` the test set under the training set - To the same scale**\n",
    "\n",
    "**BUT, fit only using training set (scaler), to treat test set as brand new set, and `does not contribute` to feature scaling formula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit will just get the mean & s.d. of each of the features.\n",
    "# Transform will apply the formula, and actually manipulate the values.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # Standardization\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "# We need to apply the SAME scaler on training set onto test set\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 -0.7529426005471072 -0.6260377781240918]\n",
      " [1.0 0.0 0.0 1.008453807952985 1.0130429500553495]\n",
      " [1.0 0.0 0.0 1.7912966561752484 1.8325833141450703]\n",
      " [0.0 1.0 0.0 -1.7314961608249362 -1.0943465576039322]\n",
      " [1.0 0.0 0.0 -0.3615211764359756 0.42765697570554906]\n",
      " [0.0 1.0 0.0 0.22561095973072184 0.05040823668012247]\n",
      " [0.0 0.0 1.0 -0.16581046438040975 -0.27480619351421154]\n",
      " [0.0 0.0 1.0 -0.013591021670525094 -1.3285009473438525]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 2.1827180802863797 2.3008920936249107]\n",
      " [0.0 0.0 1.0 -2.3186282969916334 -1.7968097268236927]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science_playground]",
   "language": "python",
   "name": "conda-env-data_science_playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
